# Example
## Helpful resources

| Description | Link |
|-------------|---------|
| Introduction to basic Lean proofs through number theory - see attached progress file | https://adam.math.hhu.de/#/g/leanprover-community/NNG4 |
| Lean Linalg proof repository | https://www.jacobserfaty.com/linear-algebra-in-lean-4 |
| Mathematics In Lean - chapter on Linear Algebra | https://leanprover-community.github.io/mathematics_in_lean/C09_Linear_Algebra.html#matrices-bases-and-dimension |
|The Mechanics of Proof - chapter on Functions | https://hrmacbeth.github.io/math2001/08_Functions.html#injectivity-and-surjectivity |
| Lean repository following LADR by Axler, uncertain correctness/completeness | https://github.com/ssomayyajula/linear/blob/master/vector_space.lean|
| List of missing areas in MathLib - note that vector calculus is entirely missing | https://leanprover-community.github.io/undergrad_todo.html |
| Xena project - collaboration from ICL to formalize undergrad math in Lean, potentially useful for how to present Lean results to a general audience | https://xenaproject.wordpress.com/2019/02/11/lean-in-latex/ |
| Library of explained/informalized MathLib linear algebra proofs | https://www.overleaf.com/read/nvmgccgfhykw#3c063b |

## Lit review/papers to cite

| Title | Link | Comments/Summary | 
|-------------|---------|-----------|
| Aesop: White-Box Best-First Proof Search for Lean (2023)| https://dl.acm.org/doi/abs/10.1145/3573105.3575671 | Conventional (non-ML) automatic proof tool, uses best-first backtracking search and over an user-specified set of tactics. This is basically a more advanced version of tactics like `tidy` bundled with MathLib. The search tree could be a helpful for some use-cases, like mapping out all possible intermediate steps/states in a proof to see where a student has gone wrong. |
| Autoformalization with Large Language Models (2022)| https://arxiv.org/pdf/2205.12615 | Presents autoformalization using LLMs in Isabelle, using few-shot in-context learning (no additional model training). Success rate is low (approx 25%), authors report that most errors are from syntax errors in invoking Isabelle (e.g. being unable to invoke fact n for n!), which might be fixable with fine-tuning. Could also see better performance by using more than 2 examples in context window, or selecting examples for relevancy. Also tests informalization with good (76%) success rate. |
| LeanDojo: Theorem Proving in Lean using Language Models (2023) | https://arxiv.org/pdf/2306.15626 | Has LeanDojo, a potentially helpful tool for tracing Lean repos and interfacing with neural networks. Also has Reprover, a next-tactic suggestion tool based on context retrieval and a transformer. Suggested tactics are incorporated into best-first search to ultimately generate proofs. |
| TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts (2024) | https://arxiv.org/pdf/2407.03203v1 | Uses an end-to-end LLM approach (as opposed to next-tactic suggestion and search), tries to generate a formal Lean proof given informal proof. Has interesting work on natural language/formal language alignment (e.g. asking the model to add comments to Lean code) and a possible Mathlib deformalization approach by in-context learning. Seems less applicable because this assumes the existence of informal proofs and doesn't generate a tree-like structure, but could still be helpful.|
| Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs (2023) | https://arxiv.org/pdf/2210.12283 | Generates a formal proof from an informal sketch (including a number of intermediate steps) using Isabelle, seems similar to what TheoremLlama does but with slightly different implementation (has human/informal LLM split the proof into distinct steps to formalize separately instead of NL/FL bootstrapping) |
| Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers (2022) | https://arxiv.org/pdf/2205.10893 | Designs a tool that does premise selection by learning when to apply "hammers" in Isabelle, instead of doing premise selection directly with the LLM. This is incorprated into a best-first search approach. This has better success (57% on PISA/test vs 39% with LLM only), but applicability is limited because Lean does not appear to have as good of support for hammers. This paper remarks that Isabelle hammers are more successful at premise selection than LLM methods. In particular, Sledgehammer gives large numbers of (possibly relevant) premises to an external ATP, and keeps the premises that the ATP deems to be helpful.
|A Survey on Deep Learning for Theorem Proving (2024)|https://arxiv.org/pdf/2404.09939||
|An In-Context Learning Agent for Formal Theorem-Proving (2024) |https://arxiv.org/pdf/2310.04353| Presents the COPRA method, which uses a general-purpose LLM (GPT-4) to generate proof tactics, maintaining a backtracking search tree. Performance is somewhat better than Reprover and comparable with Thor on the mini-F2F dataset. The search tree is useful, but the general setup seems somewhat simple and might be subject to low-hanging improvements from fine-tuning etc.|
|Llemma: An Open Language Model For Mathematics (2024) | https://arxiv.org/pdf/2310.10631| A LLM trained on a mix of domain-specific math content, including web crawl pages filtered for math content and AlgebraicStack, a collection of 11b tokens from 17 symbolic equations. They benchmark on some few-shot theorem proving tasks and find similar performance as Reprover/Sledgehammer, without using any additional steps or context retrieval. 
|Multilingual Mathematical Autoformalization |  https://arxiv.org/pdf/2311.03755 | Uses GPT4 to informalize the statements in the archive of formal proofs and Mathlib4, making formal-informal pairs (dataset is publicly available). Fine-tuned LLama on the dataset with reasonable but not great results in autoformalization (30% of test set from miniF2F and ProofNet needed moderate human correction or less). Dataset could definitely be useful, the the quality is suspect due to GPT4 not formalizing well. |
| Towards Large Language Models as Copilots for Theorem Proving in Lean (2024)| https://arxiv.org/pdf/2404.12534 | More of an implementational paper discussing Lean Copilot, which combines Reprover next-tactic suggestion with Aesop. Notes that they modify Aesop to draw tactics from an external LLM, instead of using a fixed pre-specified set of tactics. |
| Autoformalizing Mathematical Statements by Symbolic Equivalence and Semantic Consistency (2024) | https://arxiv.org/pdf/2410.20936 | Considers improving LLM autoformalization performance by generating multiple different autoformalizations and evaluating them based on the metrics of Symbolic Equivalence and Semantic Consistency. Symbolic equivalence is if candidates are equal to each other logically, checked with an automatic theorem prover. Semantic consistiency is informal => formal => informal and using an embedding model (like BERT) to calculate the similarity between the informal proofs. Somewhat similar to our idea, but is limited to theorem statements (and not proofs), ideas could be applied in combination with other approaches.|


### Discussion of Isabelle Hammers

| Title | Link | Comments/Summary | 
|---------|---------|------------|
|Automation for Interactive Proof: First Prototype (2005)| https://www.cl.cam.ac.uk/~lp15/papers/Automation/info-and-comp.pdf | Discussion of the first version of Sledgehammer (predating the name). Sledgehammer is a way to seamlessly interface Isabelle with automatic theorem provers (ATPs) that only work with basic (first-order, untyped, clause form) logic, such as Vampire and SPASS. The main engineering challenges are developing a way to seamlessly integrate Isabelle states (e.g. existing lemmas, goals, assumptions) with the ATP, and translating Isabelle states to first-order clauses. Then, the user can run the ATP directly from higher-order theorems in Isabelle, whilst also manipulating statements from Isabelle.|
|Sledgehammer: some history, some tips (2022)|https://lawrencecpaulson.github.io/2022/04/13/Sledgehammer.html| A similar overview of sledgehammers in Isabelle|
|Three Years of Experience with Sledgehammer, a Practical Link between Automatic and Interactive Theorem Provers (2015)|https://www.cl.cam.ac.uk/~lp15/papers/Automation/paar.pdf| Discusses additional features of Sledgehammer, including translation to first-order logic, lemma filtering (context retrieval)|
|First-Order Theorem Proving and Vampire (2013)| https://publik.tuwien.ac.at/files/PubDat_225994.pdf | A manual describing how to use the Vampire tool. The survey (Li, 2024) describes Vampire as a saturation-based prover that generates brute-force implications from a first-order logic statement until a proof/resolution or timeout is reached. |
